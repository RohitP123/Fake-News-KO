{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy0Q0rvh52Xl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def date_range_chunks(start_date_str, end_date_str, num_chunks=10):\n",
        "    \"\"\"\n",
        "    Split a date range into smaller chunks.\n",
        "\n",
        "    Args:\n",
        "        start_date_str (str): Start date in format 'YYYY-MM-DD'\n",
        "        end_date_str (str): End date in format 'YYYY-MM-DD'\n",
        "        num_chunks (int): Number of chunks to create\n",
        "\n",
        "    Returns:\n",
        "        list: List of date range tuples (start_date, end_date) as strings\n",
        "    \"\"\"\n",
        "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
        "\n",
        "    # Calculate the total number of days\n",
        "    total_days = (end_date - start_date).days\n",
        "\n",
        "    # Adjust num_chunks if it's more than the total days\n",
        "    num_chunks = min(num_chunks, total_days)\n",
        "\n",
        "    if num_chunks <= 1:\n",
        "        return [(start_date_str, end_date_str)]\n",
        "\n",
        "    # Calculate days per chunk (can be a float for more even distribution)\n",
        "    days_per_chunk = total_days / num_chunks\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        chunk_start = start_date + timedelta(days=(i * days_per_chunk))\n",
        "        # If it's the last chunk, use the exact end date\n",
        "        if i == num_chunks - 1:\n",
        "            chunk_end = end_date\n",
        "        else:\n",
        "            chunk_end = start_date + timedelta(days=((i + 1) * days_per_chunk)) - timedelta(seconds=1)\n",
        "\n",
        "        # Format dates as strings\n",
        "        chunk_start_str = chunk_start.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        chunks.append((chunk_start_str, chunk_end_str))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def fetch_news_articles(api_key, start_date, end_date, news_sources, language=\"en\", page_size=100,\n",
        "                       max_articles_per_source_chunk=25, date_chunks_count=5):\n",
        "    \"\"\"\n",
        "    Fetch news articles from WorldNewsApi with date range chunking and multiple news sources.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Your WorldNewsApi API key\n",
        "        start_date (str): Earliest publish date in format YYYY-MM-DD\n",
        "        end_date (str): Latest publish date in format YYYY-MM-DD\n",
        "        news_sources (list): List of news source URLs\n",
        "        language (str): Language code (default: 'en')\n",
        "        page_size (int): Number of results per page (default: 100, max allowed)\n",
        "        max_articles_per_source_chunk (int): Maximum articles to fetch per source per date chunk\n",
        "        date_chunks_count (int): Number of date chunks to divide the time period into\n",
        "\n",
        "    Returns:\n",
        "        list: List of news article dictionaries\n",
        "    \"\"\"\n",
        "    base_url = \"https://api.worldnewsapi.com/search-news\"\n",
        "\n",
        "    all_articles = []\n",
        "\n",
        "    # Calculate date range chunks\n",
        "    date_chunks = date_range_chunks(start_date, end_date, num_chunks=date_chunks_count)\n",
        "\n",
        "    headers = {\n",
        "        'x-api-key': api_key\n",
        "    }\n",
        "\n",
        "    # Keep track of API calls for reporting\n",
        "    total_api_calls = 0\n",
        "\n",
        "    # Cycle through each date chunk and each news source\n",
        "    for idx, (chunk_start, chunk_end) in enumerate(date_chunks):\n",
        "        print(f\"\\nProcessing date chunk {idx+1}/{len(date_chunks)}: {chunk_start} to {chunk_end}\")\n",
        "\n",
        "        for source_idx, news_source in enumerate(news_sources):\n",
        "            print(f\"  News source {source_idx+1}/{len(news_sources)}: {news_source}\")\n",
        "\n",
        "            current_offset = 0\n",
        "            chunk_source_articles = []\n",
        "\n",
        "            # Fetch up to max_articles_per_source_chunk articles per source per date chunk\n",
        "            while len(chunk_source_articles) < max_articles_per_source_chunk:\n",
        "                # Calculate remaining articles to fetch for this source/chunk\n",
        "                remaining = max_articles_per_source_chunk - len(chunk_source_articles)\n",
        "                current_page_size = min(page_size, remaining)\n",
        "\n",
        "                # Build the query parameters\n",
        "                params = {\n",
        "                    'language': language,\n",
        "                    'earliest-publish-date': chunk_start,\n",
        "                    'latest-publish-date': chunk_end,\n",
        "                    'news-sources': news_source,\n",
        "                    'number': current_page_size,\n",
        "                    'offset': current_offset,\n",
        "                    'sort': 'publish-time',\n",
        "                    'sort-direction': 'DESC'\n",
        "                }\n",
        "\n",
        "                print(f\"    Fetching up to {current_page_size} articles with offset {current_offset}...\")\n",
        "\n",
        "                try:\n",
        "                    # Make the API call\n",
        "                    response = requests.get(base_url, headers=headers, params=params)\n",
        "                    total_api_calls += 1\n",
        "\n",
        "                    # Handle rate limiting\n",
        "                    if response.status_code == 429:\n",
        "                        wait_time = int(response.headers.get('Retry-After', 60))\n",
        "                        print(f\"    Rate limit reached. Waiting for {wait_time} seconds...\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue\n",
        "\n",
        "                    # Handle other errors\n",
        "                    if response.status_code != 200:\n",
        "                        print(f\"    Error: {response.status_code} - {response.text}\")\n",
        "                        break\n",
        "\n",
        "                    data = response.json()\n",
        "                    articles = data.get('news', [])\n",
        "\n",
        "                    # Add source identifier to help with analysis\n",
        "                    for article in articles:\n",
        "                        article['fetched_from'] = news_source\n",
        "\n",
        "                    # If no more articles, break the loop\n",
        "                    if not articles:\n",
        "                        print(\"    No more articles found for this source in this date range.\")\n",
        "                        break\n",
        "\n",
        "                    chunk_source_articles.extend(articles)\n",
        "                    print(f\"    Retrieved {len(articles)} articles. Total for this source/chunk: {len(chunk_source_articles)}\")\n",
        "\n",
        "                    # Update offset for the next page\n",
        "                    current_offset += len(articles)\n",
        "\n",
        "                    # Check if we've reached the total number of available news\n",
        "                    total_news = data.get('available', 0)\n",
        "                    if current_offset >= total_news:\n",
        "                        print(f\"    Reached the end of available news for this source/date range ({total_news} total).\")\n",
        "                        break\n",
        "\n",
        "                    # Short delay to avoid hammering the API\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error fetching data: {str(e)}\")\n",
        "                    break\n",
        "\n",
        "            all_articles.extend(chunk_source_articles)\n",
        "            print(f\"  Completed source {source_idx+1}/{len(news_sources)}. Articles from this source: {len(chunk_source_articles)}\")\n",
        "\n",
        "        print(f\"Completed date chunk {idx+1}/{len(date_chunks)}. Total articles so far: {len(all_articles)}\")\n",
        "\n",
        "    print(f\"\\nTotal API calls made: {total_api_calls}\")\n",
        "    return all_articles\n",
        "\n",
        "def analyze_distribution(articles):\n",
        "    \"\"\"\n",
        "    Analyze and print the distribution of articles by date and source.\n",
        "\n",
        "    Args:\n",
        "        articles (list): List of article dictionaries\n",
        "    \"\"\"\n",
        "    date_counts = {}\n",
        "    source_counts = {}\n",
        "    source_date_counts = {}\n",
        "\n",
        "    for article in articles:\n",
        "        # Extract source\n",
        "        source = article.get('fetched_from', 'unknown')\n",
        "\n",
        "        # Extract publish date\n",
        "        publish_date = article.get('publish_date', '')\n",
        "        if publish_date:\n",
        "            # Extract just the date part (YYYY-MM-DD)\n",
        "            date_only = publish_date.split('T')[0] if 'T' in publish_date else publish_date.split(' ')[0]\n",
        "\n",
        "            # Update counts\n",
        "            date_counts[date_only] = date_counts.get(date_only, 0) + 1\n",
        "            source_counts[source] = source_counts.get(source, 0) + 1\n",
        "\n",
        "            # Create combined key for source and date\n",
        "            source_date_key = f\"{source}|{date_only}\"\n",
        "            source_date_counts[source_date_key] = source_date_counts.get(source_date_key, 0) + 1\n",
        "\n",
        "    # Print source distribution\n",
        "    print(\"\\nArticle Distribution by Source:\")\n",
        "    for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{source}: {count} articles\")\n",
        "\n",
        "    # Print date distribution\n",
        "    print(\"\\nArticle Distribution by Date:\")\n",
        "    for date, count in sorted(date_counts.items()):\n",
        "        print(f\"{date}: {count} articles\")\n",
        "\n",
        "    # Print source-date distribution (top 10)\n",
        "    print(\"\\nTop Source-Date Combinations:\")\n",
        "    top_source_dates = sorted(source_date_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    for source_date, count in top_source_dates:\n",
        "        source, date = source_date.split('|')\n",
        "        print(f\"{date} from {source}: {count} articles\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    if date_counts:\n",
        "        total_dates = len(date_counts)\n",
        "        total_sources = len(source_counts)\n",
        "        avg_articles_per_date = sum(date_counts.values()) / total_dates\n",
        "        avg_articles_per_source = sum(source_counts.values()) / total_sources\n",
        "\n",
        "        print(f\"\\nDistribution Statistics:\")\n",
        "        print(f\"- Total sources covered: {total_sources}\")\n",
        "        print(f\"- Total dates covered: {total_dates}\")\n",
        "        print(f\"- Avg articles per source: {avg_articles_per_source:.2f}\")\n",
        "        print(f\"- Avg articles per date: {avg_articles_per_date:.2f}\")\n",
        "\n",
        "def save_to_csv(articles, filename):\n",
        "    \"\"\"\n",
        "    Save the articles to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        articles (list): List of article dictionaries\n",
        "        filename (str): Output CSV filename\n",
        "    \"\"\"\n",
        "    if not articles:\n",
        "        print(\"No articles to save.\")\n",
        "        return\n",
        "\n",
        "    # Extract all possible keys from all articles to use as CSV headers\n",
        "    fieldnames = set()\n",
        "    for article in articles:\n",
        "        fieldnames.update(article.keys())\n",
        "\n",
        "    fieldnames = sorted(list(fieldnames))\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(articles)\n",
        "\n",
        "    print(f\"Saved {len(articles)} articles to {filename}\")\n"
      ],
      "metadata": {
        "id": "eKw-yNtG5_Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # API key (replace with your actual API key)\n",
        "    api_key = \"2eefb6ac28254d0faf6698987a6d7e46\"\n",
        "\n",
        "    # Date range (March 1, 2025 to April 1, 2025)\n",
        "    start_date = \"2024-03-20\"\n",
        "    end_date = \"2025-01-01\"\n",
        "\n",
        "    # News sources\n",
        "    news_sources = [\n",
        "        \"https://www.bbc.co.uk\",\n",
        "        \"https://politicalwire.com\",\n",
        "        \"https://www.nytimes.com\"\n",
        "    ]\n",
        "\n",
        "    # Language\n",
        "    language = \"en\"\n",
        "\n",
        "    # Number of results per page (maximum allowed is 100)\n",
        "    page_size = 100\n",
        "\n",
        "    # Maximum articles to fetch per source per date chunk\n",
        "    # Lower number means more even distribution across sources and dates\n",
        "    max_articles_per_source_chunk = 100\n",
        "\n",
        "    # Number of date chunks to divide the time period into\n",
        "    date_chunks_count = 8\n",
        "\n",
        "    # Generate filename with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"news_articles_{start_date}_to_{end_date}_{timestamp}.csv\"\n",
        "\n",
        "    print(f\"Fetching news articles from {start_date} to {end_date} from {len(news_sources)} sources...\")\n",
        "    articles = fetch_news_articles(api_key, start_date, end_date, news_sources,\n",
        "                                  language, page_size, max_articles_per_source_chunk, date_chunks_count)\n",
        "\n",
        "    print(f\"\\nTotal articles retrieved: {len(articles)}\")\n",
        "\n",
        "    # Analyze distribution\n",
        "    analyze_distribution(articles)\n",
        "\n",
        "    # Save to CSV\n",
        "    save_to_csv(articles, filename)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PeLHza2B3Jfo",
        "outputId": "5442266e-09ed-4080-a982-f16c141418c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching news articles from 2024-03-20 to 2025-01-01 from 4 sources...\n",
            "\n",
            "Processing date chunk 1/8: 2024-03-20 to 2024-04-24\n",
            "  News source 1/4: https://www.bbc.co.uk\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 1/4. Articles from this source: 0\n",
            "  News source 2/4: https://politicalwire.com\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 2/4. Articles from this source: 0\n",
            "  News source 3/4: https://www.foxnews.com\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 3/4. Articles from this source: 0\n",
            "  News source 4/4: https://www.nytimes.com\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 4/4. Articles from this source: 0\n",
            "Completed date chunk 1/8. Total articles so far: 0\n",
            "\n",
            "Processing date chunk 2/8: 2024-04-24 to 2024-05-30\n",
            "  News source 1/4: https://www.bbc.co.uk\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 1/4. Articles from this source: 0\n",
            "  News source 2/4: https://politicalwire.com\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 2/4. Articles from this source: 0\n",
            "  News source 3/4: https://www.foxnews.com\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 3/4. Articles from this source: 0\n",
            "  News source 4/4: https://www.nytimes.com\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 4/4. Articles from this source: 0\n",
            "Completed date chunk 2/8. Total articles so far: 0\n",
            "\n",
            "Processing date chunk 3/8: 2024-05-30 to 2024-07-05\n",
            "  News source 1/4: https://www.bbc.co.uk\n",
            "    Fetching up to 100 articles with offset 0...\n",
            "    Error: 400 - {\"status\":\"failure\", \"code\":400,\"message\":\"On the free and starter plan, you cannot look back further than 1 month, please set 'earliest-publish-date' to a date within the last 30 days.\"}\n",
            "  Completed source 1/4. Articles from this source: 0\n",
            "  News source 2/4: https://politicalwire.com\n",
            "    Fetching up to 100 articles with offset 0...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7fefdd62075c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7fefdd62075c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fetching news articles from {start_date} to {end_date} from {len(news_sources)} sources...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     articles = fetch_news_articles(api_key, start_date, end_date, news_sources,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                   language, page_size, max_articles_per_source_chunk, date_chunks_count)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-d00f07cde827>\u001b[0m in \u001b[0;36mfetch_news_articles\u001b[0;34m(api_key, start_date, end_date, news_sources, language, page_size, max_articles_per_source_chunk, date_chunks_count)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0;31m# Make the API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtotal_api_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}