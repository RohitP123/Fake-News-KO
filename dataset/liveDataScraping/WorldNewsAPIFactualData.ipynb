{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy0Q0rvh52Xl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_news_articles(api_key, start_date, end_date, news_source, language=\"en\", page_size=100, max_pages=10):\n",
        "    \"\"\"\n",
        "    Fetch news articles from WorldNewsApi with pagination.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Your WorldNewsApi API key\n",
        "        start_date (str): Earliest publish date in format YYYY-MM-DD\n",
        "        end_date (str): Latest publish date in format YYYY-MM-DD\n",
        "        news_source (str): URL of the news source\n",
        "        language (str): Language code (default: 'en')\n",
        "        page_size (int): Number of results per page (default: 100, max allowed)\n",
        "        max_pages (int): Maximum number of pages to fetch\n",
        "\n",
        "    Returns:\n",
        "        list: List of news article dictionaries\n",
        "    \"\"\"\n",
        "    base_url = \"https://api.worldnewsapi.com/search-news\"\n",
        "\n",
        "    all_articles = []\n",
        "    current_offset = 0\n",
        "\n",
        "    headers = {\n",
        "        'x-api-key': api_key\n",
        "    }\n",
        "\n",
        "    for page in range(max_pages):\n",
        "        # Build the query parameters\n",
        "        params = {\n",
        "            'language': language,\n",
        "            'earliest-publish-date': start_date,\n",
        "            'latest-publish-date': end_date,\n",
        "            # 'news-sources': news_source,\n",
        "            'number': page_size,\n",
        "            'offset': current_offset,\n",
        "            'sort': 'publish-time',\n",
        "            'sort-direction': 'DESC'\n",
        "        }\n",
        "\n",
        "        print(f\"Fetching page {page+1} with offset {current_offset}...\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url, headers=headers, params=params)\n",
        "\n",
        "            # Handle rate limiting\n",
        "            if response.status_code == 429:\n",
        "                wait_time = int(response.headers.get('Retry-After', 60))\n",
        "                print(f\"Rate limit reached. Waiting for {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "\n",
        "            # Handle other errors\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: {response.status_code} - {response.text}\")\n",
        "                break\n",
        "\n",
        "            data = response.json()\n",
        "            articles = data.get('news', [])\n",
        "\n",
        "            # If no more articles, break the loop\n",
        "            if not articles:\n",
        "                print(\"No more articles found.\")\n",
        "                break\n",
        "\n",
        "            all_articles.extend(articles)\n",
        "            print(f\"Retrieved {len(articles)} articles. Total so far: {len(all_articles)}\")\n",
        "\n",
        "            # Update offset for the next page\n",
        "            current_offset += page_size\n",
        "\n",
        "            # Check if we've reached the total number of available news\n",
        "            total_news = data.get('available', 0)\n",
        "            if current_offset >= total_news:\n",
        "                print(f\"Reached the end of available news ({total_news} total).\")\n",
        "                break\n",
        "\n",
        "            # Short delay to avoid hammering the API\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return all_articles\n"
      ],
      "metadata": {
        "id": "eKw-yNtG5_Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(articles, filename):\n",
        "    \"\"\"\n",
        "    Save the articles to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        articles (list): List of article dictionaries\n",
        "        filename (str): Output CSV filename\n",
        "    \"\"\"\n",
        "    if not articles:\n",
        "        print(\"No articles to save.\")\n",
        "        return\n",
        "\n",
        "    # Extract all possible keys from all articles to use as CSV headers\n",
        "    fieldnames = set()\n",
        "    for article in articles:\n",
        "        fieldnames.update(article.keys())\n",
        "\n",
        "    fieldnames = sorted(list(fieldnames))\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(articles)\n",
        "\n",
        "    print(f\"Saved {len(articles)} articles to {filename}\")\n"
      ],
      "metadata": {
        "id": "6k7Nbqq-5_xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # API key (replace with your actual API key)\n",
        "    api_key = \"6f5ecd01552340ebad22daa8344947fc\"\n",
        "\n",
        "    # Date range (March 1, 2025 to April 1, 2025)\n",
        "    start_date = \"2025-03-05\"\n",
        "    end_date = \"2025-04-01\"\n",
        "\n",
        "    # News source\n",
        "    news_source = \"https://www.bbc.co.uk\"\n",
        "\n",
        "    # Language\n",
        "    language = \"en\"\n",
        "\n",
        "    # Number of results per page (maximum allowed is 100)\n",
        "    page_size = 100\n",
        "\n",
        "    # Maximum number of pages to fetch (adjust based on your API limits)\n",
        "    # For example, if your plan allows 1000 calls, set this to 10 to get 1000 articles\n",
        "    max_pages = 100\n",
        "\n",
        "    # Generate filename with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"bbc_news_{start_date}_to_{end_date}_{timestamp}.csv\"\n",
        "\n",
        "    print(f\"Fetching BBC news articles from {start_date} to {end_date}...\")\n",
        "    articles = fetch_news_articles(api_key, start_date, end_date, news_source,\n",
        "                                   language, page_size, max_pages)\n",
        "\n",
        "    print(f\"Total articles retrieved: {len(articles)}\")\n",
        "    save_to_csv(articles, filename)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeLHza2B3Jfo",
        "outputId": "14ad1d50-fc54-450e-b194-54bdaa808257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching BBC news articles from 2025-03-05 to 2025-04-01...\n",
            "Fetching page 1 with offset 0...\n",
            "Retrieved 100 articles. Total so far: 100\n",
            "Fetching page 2 with offset 100...\n",
            "Retrieved 100 articles. Total so far: 200\n",
            "Fetching page 3 with offset 200...\n",
            "Retrieved 100 articles. Total so far: 300\n",
            "Fetching page 4 with offset 300...\n",
            "Retrieved 100 articles. Total so far: 400\n",
            "Fetching page 5 with offset 400...\n",
            "Retrieved 100 articles. Total so far: 500\n",
            "Fetching page 6 with offset 500...\n",
            "Retrieved 100 articles. Total so far: 600\n",
            "Fetching page 7 with offset 600...\n",
            "Retrieved 100 articles. Total so far: 700\n",
            "Fetching page 8 with offset 700...\n",
            "Retrieved 100 articles. Total so far: 800\n",
            "Fetching page 9 with offset 800...\n",
            "Retrieved 100 articles. Total so far: 900\n",
            "Fetching page 10 with offset 900...\n",
            "Retrieved 100 articles. Total so far: 1000\n",
            "Fetching page 11 with offset 1000...\n",
            "Retrieved 100 articles. Total so far: 1100\n",
            "Fetching page 12 with offset 1100...\n",
            "Retrieved 100 articles. Total so far: 1200\n",
            "Fetching page 13 with offset 1200...\n",
            "Retrieved 100 articles. Total so far: 1300\n",
            "Fetching page 14 with offset 1300...\n",
            "Retrieved 100 articles. Total so far: 1400\n",
            "Fetching page 15 with offset 1400...\n",
            "Retrieved 100 articles. Total so far: 1500\n",
            "Fetching page 16 with offset 1500...\n",
            "Retrieved 100 articles. Total so far: 1600\n",
            "Fetching page 17 with offset 1600...\n",
            "Retrieved 100 articles. Total so far: 1700\n",
            "Fetching page 18 with offset 1700...\n",
            "Retrieved 100 articles. Total so far: 1800\n",
            "Fetching page 19 with offset 1800...\n",
            "Retrieved 100 articles. Total so far: 1900\n",
            "Fetching page 20 with offset 1900...\n",
            "Retrieved 100 articles. Total so far: 2000\n",
            "Fetching page 21 with offset 2000...\n",
            "Retrieved 100 articles. Total so far: 2100\n",
            "Fetching page 22 with offset 2100...\n",
            "Retrieved 100 articles. Total so far: 2200\n",
            "Fetching page 23 with offset 2200...\n",
            "Retrieved 100 articles. Total so far: 2300\n",
            "Fetching page 24 with offset 2300...\n",
            "Error: 402 - {\"status\":\"failure\", \"code\":402,\"message\":\"Your daily points limit of 50 has been reached. Please upgrade your plan to continue using the API.\"}\n",
            "Total articles retrieved: 2300\n",
            "Saved 2300 articles to bbc_news_2025-03-05_to_2025-04-01_20250402_235607.csv\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}