{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4rCD4J_g_Tv",
        "outputId": "bbb0d158-e6ca-48f2-c636-9a7500858ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, faiss-cpu, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 faiss-cpu-1.10.0 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch scikit-learn pandas matplotlib faiss-cpu sentence-transformers\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Liar2_combined.csv\", header = 0)\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d', errors='coerce')\n",
        "df = df.dropna(subset=['date'])\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYy0pVPwjk9F",
        "outputId": "bb934d9e-198d-49ef-88ed-70f7b37389f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                              title       date\n",
            "0      1  90 percent of Americans \"support universal bac... 2017-10-02\n",
            "1      0  Last year was one of the deadliest years ever ... 2017-05-19\n",
            "2      0  Bernie Sanders's plan is \"to raise your taxes ... 2015-10-28\n",
            "3      1  Voter ID is supported by an overwhelming major... 2021-12-08\n",
            "4      0  Says Barack Obama \"robbed Medicare (of) $716 b... 2012-08-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Defining our Date Ranges\n",
        "baseline_start, baseline_end = '2007-01-01', '2015-12-31'\n",
        "update1_start, update1_end   = '2016-01-01', '2017-12-31'\n",
        "update2_start, update2_end   = '2018-01-01', '2019-12-31'\n",
        "update3_start, update3_end   = '2020-01-01', '2021-12-31'\n",
        "update4_start, update4_end   = '2022-01-01', '2022-12-31'\n",
        "test_start, test_end         = '2023-01-01', '2023-12-31'\n",
        "\n",
        "# Baseline training set: entries w/ date <= split_date\n",
        "baseline_df = df[(df['date'] >= baseline_start) & (df['date'] <= baseline_end)].copy()\n",
        "update1_df = df[(df['date'] >= update1_start) & (df['date'] <= update1_end)].copy()\n",
        "update2_df = df[(df['date'] >= update2_start) & (df['date'] <= update2_end)].copy()\n",
        "update3_df = df[(df['date'] >= update3_start) & (df['date'] <= update3_end)].copy()\n",
        "update4_df = df[(df['date'] >= update4_start) & (df['date'] <= update4_end)].copy()\n",
        "test_df = df[(df['date'] >= test_start) & (df['date'] <= test_end)].copy()\n",
        "\n",
        "# Display sample sizes for each block\n",
        "print(\"Baseline samples:\", len(baseline_df))\n",
        "print(\"Update 1 samples:\", len(update1_df))\n",
        "print(\"Update 2 samples:\", len(update2_df))\n",
        "print(\"Update 3 samples:\", len(update3_df))\n",
        "print(\"Update 4 samples:\", len(update4_df))\n",
        "print(\"Test samples:\", len(test_df))\n",
        "\n",
        "print(baseline_df.head())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94D9c_rhjuPG",
        "outputId": "cb6ee2e5-c1d5-4377-f073-3a6a1cf44e04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline samples: 10932\n",
            "Update 1 samples: 3031\n",
            "Update 2 samples: 2730\n",
            "Update 3 samples: 3772\n",
            "Update 4 samples: 1688\n",
            "Test samples: 807\n",
            "    label                                              title       date\n",
            "2       0  Bernie Sanders's plan is \"to raise your taxes ... 2015-10-28\n",
            "4       0  Says Barack Obama \"robbed Medicare (of) $716 b... 2012-08-12\n",
            "6       0  Says Jeff Reardon cut elementary school music ... 2012-05-08\n",
            "11      0  Says PolitiFact \"listed Governor Scott Walker ... 2012-06-04\n",
            "12      1  Guantanamo has \"never been a key component of ... 2015-12-27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_headlines = pd.read_csv(\"headlines.csv\", header=0)\n",
        "\n",
        "# Convert the Date column (YYYYMMDD) to a datetime object and drop rows where parsing fails.\n",
        "df_headlines['parsed_date'] = pd.to_datetime(df_headlines['Date'], format='%Y%m%d', errors='coerce')\n",
        "df_headlines = df_headlines.dropna(subset=['parsed_date'])\n",
        "\n",
        "print(\"Columns:\", df_headlines.columns.tolist())\n",
        "print(\"CSV sample:\")\n",
        "print(df_headlines.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJDWroVujwng",
        "outputId": "2ec2b338-c033-4fde-affc-5358011226d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['Date', 'Publication', 'Headline', 'URL', 'parsed_date']\n",
            "CSV sample:\n",
            "       Date     Publication  \\\n",
            "0  20070101  New York Times   \n",
            "1  20070101  New York Times   \n",
            "2  20070101  New York Times   \n",
            "3  20070101  New York Times   \n",
            "4  20070101  New York Times   \n",
            "\n",
            "                                            Headline  \\\n",
            "0               Rush to Hang Hussein Was  Questioned   \n",
            "1  News Analysis: For Sunnis, Dictators End Is O...   \n",
            "2                            Hard Choices Over Video   \n",
            "3     States Take Lead on Ethics Rules for Lawmakers   \n",
            "4  Spitzer Arrives With Mandate, but Faces Challe...   \n",
            "\n",
            "                                                 URL parsed_date  \n",
            "0  http://www.nytimes.com/2007/01/01/world/middle...  2007-01-01  \n",
            "1  http://www.nytimes.com/2007/01/01/world/middle...  2007-01-01  \n",
            "2  http://www.nytimes.com/2007/01/01/world/middle...  2007-01-01  \n",
            "3  http://www.nytimes.com/2007/01/01/us/01ethics....  2007-01-01  \n",
            "4  http://www.nytimes.com/2007/01/01/nyregion/01e...  2007-01-01  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the filtered dataframes into a list of articles (dictionaries)\n",
        "def df_to_articles(df, headline_col=\"Headline\", date_col=\"parsed_date\"):\n",
        "    articles = []\n",
        "    for _, row in df.iterrows():\n",
        "        headline = row[headline_col]\n",
        "        # Skip if headline is not a string (e.g., NaN)\n",
        "        if not isinstance(headline, str):\n",
        "            continue\n",
        "        articles.append({\n",
        "            \"headline\": headline.strip(),\n",
        "            \"parsed_date\": row[date_col]\n",
        "        })\n",
        "    return articles"
      ],
      "metadata": {
        "id": "l1QFYy6Cpeia"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_df_sample = df_headlines[df_headlines['parsed_date'] <= baseline_end]\n",
        "if len(baseline_df_sample) > 200000:\n",
        "    baseline_df_sample = baseline_df_sample.sample(n=200000, random_state=42)\n",
        "\n",
        "\n",
        "update1_df_sample = df_headlines[(df_headlines['parsed_date'] > baseline_end) & (df_headlines['parsed_date'] <= update1_end)]\n",
        "if len(update1_df_sample) > 200000:\n",
        "    update1_df_sample = update1_df_sample.sample(n=200000, random_state=42)\n",
        "\n",
        "\n",
        "update2_df_sample = df_headlines[(df_headlines['parsed_date'] > update1_end) & (df_headlines['parsed_date'] <= update2_end)]\n",
        "if len(update2_df_sample) > 200000:\n",
        "    update2_df_sample = update2_df_sample.sample(n=200000, random_state=42)\n",
        "\n",
        "\n",
        "update3_df_sample = df_headlines[(df_headlines['parsed_date'] > update2_end) & (df_headlines['parsed_date'] <= update3_end)]\n",
        "if len(update3_df_sample) > 200000:\n",
        "    update3_df_sample = update3_df_sample.sample(n=200000, random_state=42)\n",
        "\n",
        "\n",
        "update4_df_sample = df_headlines[(df_headlines['parsed_date'] > update3_end) & (df_headlines['parsed_date'] <= update4_end)]\n",
        "if len(update4_df_sample) > 200000:\n",
        "    update4_df_sample = update4_df_sample.sample(n=200000, random_state=42)\n",
        "\n",
        "print(len(baseline_df_sample))\n",
        "print(len(update1_df_sample))\n",
        "print(len(update2_df_sample))\n",
        "print(len(update3_df_sample))\n",
        "print(len(update4_df_sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuCwCBhJkiwg",
        "outputId": "1d4aea1c-96e8-45f9-ed02-cb1ff827b89e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n",
            "200000\n",
            "200000\n",
            "200000\n",
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles_baseline = df_to_articles(baseline_df_sample)\n",
        "articles_update1  = df_to_articles(update1_df_sample)\n",
        "articles_update2  = df_to_articles(update2_df_sample)\n",
        "articles_update3  = df_to_articles(update3_df_sample)\n",
        "articles_update4  = df_to_articles(update4_df_sample)\n",
        "\n",
        "print(len(articles_baseline))\n",
        "print(len(articles_update1))\n",
        "print(len(articles_update2))\n",
        "print(len(articles_update3))\n",
        "print(len(articles_update4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ucXvq3GiOVV",
        "outputId": "827c500a-19e8-4675-fcd6-803bccde73f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197827\n",
            "198134\n",
            "198202\n",
            "198273\n",
            "198288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(articles_baseline[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP8a4FlxjRaW",
        "outputId": "dc29ba3b-aed3-414f-9d3d-98ebba0e5b1e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'headline': 'Hillary: How she needs not to beat  herself', 'parsed_date': Timestamp('2015-04-10 00:00:00')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def build_faiss_index(articles):\n",
        "    headlines = [art['headline'].lower() for art in articles]\n",
        "    embeddings = embedding_model.encode(headlines, convert_to_numpy=True)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "    return index, headlines"
      ],
      "metadata": {
        "id": "l5dZm84nkoGa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build FAISS indexes for each time period\n",
        "index_baseline, headlines_baseline = build_faiss_index(articles_baseline)\n",
        "print(\"Baseline FAISS index built with\", len(articles_baseline), \"articles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEvGbL3PkxgF",
        "outputId": "085f78ff-de78-4707-e577-ec75161de632"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline FAISS index built with 197827 articles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_update1, headlines_update1   = build_faiss_index(articles_update1)\n",
        "\n",
        "print(\"Update 1 FAISS index built with\", len(articles_update1), \"articles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HCGscM9PMfS",
        "outputId": "ce7856d5-5e60-47f6-87f1-9ca2313cfefa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update 1 FAISS index built with 198134 articles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_update2, headlines_update2   = build_faiss_index(articles_update2)\n",
        "\n",
        "print(\"Update 2 FAISS index built with\", len(articles_update2), \"articles.\")"
      ],
      "metadata": {
        "id": "9SQ_OwFzPPfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_update3, headlines_update3   = build_faiss_index(articles_update3)\n",
        "print(\"Update 3 FAISS index built with\", len(articles_update3), \"articles.\")"
      ],
      "metadata": {
        "id": "2H2-yKEYPU3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_update4, headlines_update4   = build_faiss_index(articles_update4)\n",
        "print(\"Update 4 FAISS index built with\", len(articles_update4), \"articles.\")"
      ],
      "metadata": {
        "id": "HZasW-DtPYWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_articles(query_headline, model, faiss_index, headlines, k=3):\n",
        "    query = query_headline.strip().lower()\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    distances, indices = faiss_index.search(query_embedding, k)\n",
        "    results = []\n",
        "    for rank, idx in enumerate(indices[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        hybrid_fact = f\"{headlines[idx]} (dist: {distances[0][rank]:.4f})\"\n",
        "        results.append(hybrid_fact)\n",
        "    return results"
      ],
      "metadata": {
        "id": "UtECferulsmi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_query = \"Over 4 million Americans get Omicron boosters\"\n",
        "results = search_similar_articles(test_query, embedding_model, index_update4, headlines_update4, k=3)\n",
        "print(\"Search results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYwOlu7Klvvk",
        "outputId": "19b8fa9f-7472-4224-abf5-44e467a4c55d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search results: [\"new omicron boosters are now available, but it's unclear how effective they will be (dist: 0.7497)\", \"new omicron boosters are now available, but it's unclear how effective they will be (dist: 0.7497)\", 'boosters 90% effective at preventing omicron… (dist: 0.7166)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each FAISS index to disk\n",
        "faiss.write_index(index_baseline, \"faiss_index_baseline.index\")\n",
        "faiss.write_index(index_update1, \"faiss_index_update1.index\")\n",
        "faiss.write_index(index_update2, \"faiss_index_update2.index\")\n",
        "faiss.write_index(index_update3, \"faiss_index_update3.index\")\n",
        "faiss.write_index(index_update4, \"faiss_index_update4.index\")\n",
        "\n",
        "# Compress the index files into a single zip archive\n",
        "!zip faiss_indexes.zip faiss_index_baseline.index faiss_index_update1.index faiss_index_update2.index faiss_index_update3.index faiss_index_update4.index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_gSl2btlwWa",
        "outputId": "574ba6e7-d2c8-41dd-f414-b1cde90cba93"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: faiss_index_baseline.index (deflated 8%)\n",
            "  adding: faiss_index_update1.index (deflated 7%)\n",
            "  adding: faiss_index_update2.index (deflated 8%)\n",
            "  adding: faiss_index_update3.index (deflated 7%)\n",
            "  adding: faiss_index_update4.index (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# To save the headlines:\n",
        "headlines_data = {\n",
        "    \"baseline\": headlines_baseline,\n",
        "    \"update1\": headlines_update1,\n",
        "    \"update2\": headlines_update2,\n",
        "    \"update3\": headlines_update3,\n",
        "    \"update4\": headlines_update4,\n",
        "}\n",
        "\n",
        "with open(\"faiss_headlines.pkl\", \"wb\") as f:\n",
        "    pickle.dump(headlines_data, f)\n",
        "\n",
        "print(\"Headlines saved to faiss_headlines.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_mgqHial80j",
        "outputId": "da630245-869f-489f-e5c4-960e416714e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headlines saved to faiss_headlines.pkl\n"
          ]
        }
      ]
    }
  ]
}